{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to import json support forum file\n",
    "import json \n",
    "#data frame used to orgainze data\n",
    "import pandas as pd\n",
    "#used to remove punctuation\n",
    "import re\n",
    "import string\n",
    "#natural language tool kit\n",
    "import nltk\n",
    "#used to tokenize sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#tokenizes by words. Used to tokenize words in already tokenized sentences\n",
    "from nltk.tokenize import word_tokenize\n",
    "#used in stemming and lemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#stop words \n",
    "from nltk.corpus import stopwords\n",
    "#used to perform tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "#used to graph data\n",
    "import matplotlib.pyplot as plt\n",
    "#used for exporting to csv\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(vpn):\n",
    "    \"\"\"\n",
    "       clean performs on dataset:\n",
    "            tokenizing\n",
    "            punctuation removed\n",
    "            stopwords removed\n",
    "            words lematized \n",
    "            \n",
    "        used https://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/cleaningtext.html\n",
    "    \"\"\" \n",
    "    #copies vpn to new_vpn\n",
    "    #used to save uncleaned vpn for later comparison\n",
    "    new_vpn = vpn.copy()\n",
    "    #tokenize example per sentence\n",
    "    for counter, list in enumerate(new_vpn['original_post']):\n",
    "        \n",
    "        sent_tokenize_list = sent_tokenize(list)\n",
    "        #print(sent_tokenize_list)\n",
    "        #token izes each word contained in each sentence\n",
    "        #each sentence is its own item in a list\n",
    "        tokenized_docs = [word_tokenize(doc) for doc in sent_tokenize_list]\n",
    "\n",
    "        #print('tokenizing done')\n",
    "\n",
    "        #cleans punctuation\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "        #creates new list for storage\n",
    "        tokenized_docs_no_punctuation = []\n",
    "        for review in tokenized_docs:\n",
    "            new_review = []\n",
    "            for token in review: \n",
    "                new_token = regex.sub(u'', token)\n",
    "                if not new_token == u'':\n",
    "                    new_review.append(new_token)\n",
    "\n",
    "            tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "        #print('punctuation done')\n",
    "        #removes stopwords from \n",
    "        tokenized_docs_no_stopwords = []\n",
    "        for doc in tokenized_docs_no_punctuation:\n",
    "            new_term_vector = []\n",
    "            for word in doc:\n",
    "                if not word.lower() in new_stop_list:\n",
    "                    print(word.lower())\n",
    "                    new_term_vector.append(word.lower())\n",
    "            tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "        #print('stop words done')\n",
    "        #lematizing\n",
    "        porter = PorterStemmer()\n",
    "        snowball = SnowballStemmer('english')\n",
    "        wordnet = WordNetLemmatizer()\n",
    "\n",
    "        preprocessed_docs = []\n",
    "\n",
    "        for doc in tokenized_docs_no_stopwords:\n",
    "            final_doc = []\n",
    "            for word in doc:\n",
    "                final_doc.append(porter.stem(word))\n",
    "                #final_doc.append(snowball.stem(word))\n",
    "                #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
    "            preprocessed_docs.append(final_doc)\n",
    "\n",
    "        #print('lematizing')\n",
    "        #sanatation.append(preprocessed_docs)\n",
    "        \n",
    "        #used to make sentences into strings instead of tokenized words\n",
    "        join_processed = []\n",
    "        for i in preprocessed_docs:\n",
    "            join_processed.append(\" \".join(i))\n",
    "        #print(join_processed)\n",
    "        \n",
    "        #makes sentences into one string\n",
    "        final_join = []\n",
    "        for j in join_processed:\n",
    "            final_join.append(\"\".join(j))\n",
    "        \n",
    "        final_join = \" \".join(final_join)\n",
    "        #print(final_join)\n",
    "        \n",
    "        \n",
    "        #final doc has been prepared using:\n",
    "            #tokenizing\n",
    "            #punctuation removed\n",
    "            #stopwords removed\n",
    "            #words stemmed\n",
    "        #finished with cleaning\n",
    "        #.set_value(x,y,value to be replaced)\n",
    "        \n",
    "        #join_processed sentences are tokenized, but are saved as strings\n",
    "        new_vpn.set_value(new_vpn.index[counter],'original_post',final_join)\n",
    "    print('cleaning done')\n",
    "    return new_vpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv(new_vpn): \n",
    "    new_vpn.to_csv('UCSC Dataset 3 Final - Sheet1_clean_update.csv', encoding='utf-8')\n",
    "    print('to csv done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    #create dataframe from csv of labled data\n",
    "    df = pd.read_csv('UCSC Dataset 3 Final - Sheet1.csv', encoding='utf-8')\n",
    "    df[:]\n",
    "    \n",
    "    #removes words from stop list\n",
    "    #any other words can be added to exclude\n",
    "    exclude = ['up','down', 'in', 'out', 'off', 'between']\n",
    "    new_stop_list = []\n",
    "    #creates stop word list with out words from exclude\n",
    "    for word in stopwords.words('english'):\n",
    "        if not word in exclude:\n",
    "                new_stop_list.append(word)\n",
    "    #where addition of new stop words can be added\n",
    "    new_stop_list.append('cisco')\n",
    "    new_stop_list.append('i')\n",
    "    new_stop_list.append('hi')\n",
    "    new_stop_list.append('hello')\n",
    "    new_stop_list.append('crypto')\n",
    "    print('initialization complete')\n",
    "    ret_list = []\n",
    "    ret_list.append(df)\n",
    "    ret_list.append(new_stop_list)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_list = initialize()\n",
    "vpn = create_list[0]\n",
    "new_stop_list = create_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_clean = clean(vpn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(new_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
