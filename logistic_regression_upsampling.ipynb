{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#readme\n",
    "#This file uses upsampling\n",
    "#this file splits into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in csv file\n",
    "import csv\n",
    "#vectorizes data.csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#used for dataframes\n",
    "import pandas as pd\n",
    "#used for splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "#used to perform logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#used for logistic regression metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "#used for upsampling\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads in \n",
    "df = pd.read_csv('UCSC Dataset 3 Final - Sheet1_clean.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "20\n",
      "3    124\n",
      "2     21\n",
      "0     15\n",
      "4     14\n",
      "1      6\n",
      "Name: VPN #, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Current implementation \n",
    "#splits and then runs tf-idf\n",
    "\n",
    "#used http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "train, test = train_test_split(df, test_size = .1)\n",
    "#split train set into \n",
    "print(len(train)) #180 training posts\n",
    "print(len(test)) #20\n",
    "\n",
    "#shows class distribution\n",
    "print(train['VPN #'].value_counts())\n",
    "\n",
    "#save max classifications to variable for all minority classes to be upsampled to\n",
    "new_max = max(train['VPN #'].value_counts().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represents with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 1441)\n"
     ]
    }
   ],
   "source": [
    "#creates the TF-IDF vectorizer up\n",
    "#words have to have a minimum document frequency of 2 \n",
    "#aka be in 2 documents to be included\n",
    "tfidf = TfidfVectorizer(min_df = 2)\n",
    "\n",
    "#uses vectorizer to perform fit_transform\n",
    "train_tfs = tfidf.fit_transform(train['original_post'].values.astype('U'))\n",
    "\n",
    "#transforms to fit the vocabulary of the training set\n",
    "test_tfs = tfidf.transform(test['original_post'])\n",
    "#creates a list of 20 values correlated to the test set\n",
    "num_list_test = test['VPN #'].values\n",
    "\n",
    "#Turn TF-IDF array to Pandas DataFrame\n",
    "new_train= pd.DataFrame(train_tfs.toarray())\n",
    "#Append new_train with classification values\n",
    "new_train['VPN #'] = train['VPN #'].values\n",
    "\n",
    "#shape should be (180, X)\n",
    "print(new_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling 5+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    124\n",
      "3    124\n",
      "2    124\n",
      "1    124\n",
      "0    124\n",
      "Name: VPN #, dtype: int64\n",
      "(620, 1440)\n"
     ]
    }
   ],
   "source": [
    "#up samples values to highest occurance\n",
    "#set up currently for 4 classifications\n",
    "#divides the dataframe into minority and majory dataframes\n",
    "min_zero = new_train[new_train['VPN #'] == 0]\n",
    "min_one = new_train[new_train['VPN #'] == 1]\n",
    "min_two = new_train[new_train['VPN #'] == 2]\n",
    "maj_three = new_train[new_train['VPN #'] == 3]\n",
    "min_four = new_train[new_train['VPN #'] == 4]\n",
    "\n",
    "#upsamples all but the majority\n",
    "min_zero_up = resample(min_zero, replace=True, n_samples= new_max) \n",
    "min_one_up = resample(min_one, replace=True, n_samples= new_max)\n",
    "min_two_up = resample(min_two, replace=True, n_samples= new_max) \n",
    "min_four_up = resample(min_four, replace=True, n_samples= new_max)\n",
    "\n",
    "#concatinates all of the upsampled datasets with the majority set\n",
    "new_train = pd.concat([maj_three, min_zero_up, min_one_up, min_two_up, min_four_up])\n",
    "\n",
    "#prints the value counts\n",
    "print(new_train['VPN #'].value_counts())\n",
    "#create the training set classification values\n",
    "num_list_train = new_train['VPN #'].values\n",
    "\n",
    "#drops the classification values for training purposes\n",
    "new_train = new_train.drop('VPN #', axis = 1)\n",
    "\n",
    "#shape should be (new_max*5, X-1)\n",
    "print(new_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implements Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sets up instance of logistic regression\n",
    "logistic = LogisticRegression(C = 1, solver = 'newton-cg', multi_class = 'multinomial', class_weight = 'balanced')\n",
    "#feeds in (matrix, corresponding classificaiton value)\n",
    "logistic.fit(new_train,num_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy score:', 0.69999999999999996)\n"
     ]
    }
   ],
   "source": [
    "#Runs a single fold validation\n",
    "#Result varies from run to run\n",
    "y_pred_class = logistic.predict(test_tfs)\n",
    "print(\"accuracy score:\", accuracy_score(num_list_test, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
